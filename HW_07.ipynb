{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 7**\n",
    "\n",
    "Due: **November 19, 5pm** (late submission until November 22nd, 5pm -- no submission possible afterwards)\n",
    "\n",
    "Coding assignment: 25 points\n",
    "\n",
    "Project report: 20 points\n",
    "\n",
    "### Name: [TODO]\n",
    "\n",
    "### Link to the github repo: [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evironment test below, make sure you get all green checks! If not, you will lose 2 points for each red or missing flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Programming Assignment** (25 points)\n",
    "\n",
    "### Introduction \n",
    "\n",
    "In this assignment, you will be implementing an SVM to solve binary\n",
    "classification problems. While some modern solvers use gradient-based\n",
    "methods to train the SVM, you will be using a Python quadratic\n",
    "programming library, `quadprog` as an optimizer.\n",
    "\n",
    "### Stencil Code & Data\n",
    "\n",
    "We have provided the following stencils:\n",
    "\n",
    "-   `Model` contains the `SVM` model that you will be implementing.\n",
    "\n",
    "-   `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "\n",
    "-   `QP` contains a `quadprog` wrapper function, `solve_QP`, which\n",
    "    can efficiently solve quadratic programs.\n",
    "\n",
    "-   `Main` is the entry point of your program, which will read in the\n",
    "    data, run the classifiers and print the results.\n",
    "\n",
    "You should *not* modify any code in `Main` or `QP` (with one\n",
    "exception below). The autograder will use the unmodified version of\n",
    "these two scripts. All the functions you need to fill in reside in\n",
    "`Model`, marked by `TODO`s. You can change the data path in `Main` to use\n",
    "different datasets to train the SVM.\n",
    "\n",
    "### Fake Datasets\n",
    "\n",
    "We've provided two fake datasets, `fake-data1.csv` and `fake-data2.csv`,\n",
    "for you to train your SVM classifier with. Both datasets contains only\n",
    "two dimensional data so that you can easily plot the data if you like.\n",
    "`Main` converts the labels into $\\{-1, 1\\}$. The first fake dataset\n",
    "is linearly separable while the second is not. You should use the fake\n",
    "datasets for debugging purposes.\n",
    "\n",
    "### Spambase Dataset\n",
    "\n",
    "You will also be testing your SVM classifier on a real world dataset,\n",
    "the Spambase dataset. You can find more details on the dataset\n",
    "[here](https://archive.ics.uci.edu/ml/datasets/spambase). We will\n",
    "only be using a subset of the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Assignment**\n",
    "\n",
    "### **Quadratic Program Review**\n",
    "\n",
    "Quadratic programs are a specific type of optimization problem. Each\n",
    "quadratic program consists of an objective function and a set of\n",
    "constraints. The objective function is the term that is being optimized,\n",
    "and the constraints are inequalities that limit the possible values our\n",
    "decision variables (the variables we are solving for) can take.\n",
    "\n",
    "The objective function of a quadratic program has the form:\n",
    "$$\\text{Minimize}\\frac{1}{2}x^{\\text{T}}Qx + c^{\\text{T}}x$$ \n",
    "The constraints have the form:\n",
    "\n",
    "$$Ax \\leq b$$ \n",
    "\n",
    "In these equations, Q, c, A, and b are constants that\n",
    "define the problem, and $x$ is the variable that we would like to solve\n",
    "for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**\n",
    "\n",
    "Let's see how formulating the matrices (Q and A) and vectors (c and b)\n",
    "would work in an example. Let\n",
    "$x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ be a two-dimensional\n",
    "vector. Let's say we have the following:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\text{minimize } &3x_1^2 + 2x_1x_2 + x_2^2 - 6x_1 + 4x_2 \\\\\n",
    "    \\text{subject to: }&3x_1 - x_2 \\geq 5 \\\\\n",
    "    &2x_2 \\leq 6 \\\\\n",
    "    &x_2-x_1 \\leq -1 \\\\\n",
    "    &x_1, x_2 \\geq 0 \\\\\n",
    "\\end{aligned}$$ \n",
    "We can reformat the objective function by defining $c = \\begin{pmatrix} -6 \\\\ 4 \\end{pmatrix}$ giving us $-6x_1 + 4x_2 = c^Tx$. We can also pull out a $\\frac{1}{2}$ from the quadratic terms, leaving us with:\n",
    "$$\\frac{1}{2}(6x_1^2 + 4x_1x_2 + 2x_2^2) +c^Tx$$\n",
    "\n",
    "We can define $Q = \\begin{pmatrix} 6 & z \\\\ 4-z & 2 \\end{pmatrix}$\n",
    "with $z \\in \\mathbb{R}$ (z can be any real number) giving us\n",
    "$6x_1^2 + 4x_1x_2 + 2x_2^2 = x^TQx$. Thus our objective function is now\n",
    "in the form we wish: $$\\frac{1}{2}x^TQx + c^Tx$$\\\n",
    "In order to format the inequality constraints, we must first format all\n",
    "constraints to be less than or equal to constraints. Therefore, for any\n",
    "constraint of the form $ax_1 + bx_2 \\geq c$, we must flip the inequality\n",
    "by multiplying both sides by -1. The example before would yield\n",
    "$-ax_1-bx_2 \\leq -c$. Thus we perform this on all greater than or equal\n",
    "to constraints, giving us: $$\\begin{aligned}\n",
    "    -3x_1+x_2 &\\leq -5 \\\\\n",
    "    2x_2 &\\leq 6 \\\\\n",
    "    -x_1 + x_2 &\\leq -1 \\\\\n",
    "    -x_1 &\\leq 0 \\\\\n",
    "    -x_2 &\\leq 0 \\\\\n",
    "\\end{aligned}$$ Each row of A will correspond to the coefficients of x\n",
    "of a constraint while each element of b is the constraint itself. For\n",
    "example, the first row of A could be $\\begin{pmatrix} -3 & 1 \\end{pmatrix}$\n",
    "and the first element of b would be -5. We can have \n",
    "$A = \\begin{pmatrix} -3 & 1 \\\\ 0 & 2 \\\\ -1 & 1 \\\\ -1 & 0 \\\\ 0 & -1\\end{pmatrix}$\n",
    "and $b = \\begin{pmatrix} -5 \\\\ 6 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. \n",
    "Thus our original problem is now of the form required: \n",
    "$$Ax \\leq b$$\n",
    "Thus our problem has been converted to the form we wanted, with Q, c, A,\n",
    "and b defined so that we achieve that form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quadprog**\n",
    "\n",
    "We will be using the Python library `quadprog` to solve quadratic\n",
    "programs. You do not need to call any of the methods provided by\n",
    "`quadprog` since we have written a wrapper function `solve_QP`, located\n",
    "in the `QP` file. Given $Q, c, A$, and $b$, `solve_QP` will return\n",
    "the values of $x$ that obtain the optimal solution. The example\n",
    "quadratic program from above is also included in `QP` and you can try\n",
    "running the file to find the solution.\n",
    "\n",
    "Before running the code you will need to make sure that `quadprog` is\n",
    "installed on your machine, if not, you should run `pip3 install quadprog`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SVMs as Quadratic Programs**\n",
    "\n",
    "In lecture, we showed that we can formulate the soft SVM learning\n",
    "objective with kernels as\n",
    "\n",
    "$$\\min_{\\alpha \\in \\mathbb{R}^m} \\lambda \\alpha^T G \\alpha + \\frac{1}{m}\n",
    "\\sum_{i=1}^m \\max \\{0, 1 - y_i(G \\alpha)_i \\}$$\n",
    "\n",
    "where $G$ is the Gram matrix and $\\lambda$ is a regularization\n",
    "hyperparameter. In order to use `quadprog`, we will equivalently express\n",
    "this objective as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\min_{[\\alpha, \\xi] \\in \\mathbb{R}^{2m}} \\lambda \\alpha^T G \\alpha + \\frac{1}{m} \\sum_{i=1}^m \\xi_i \\\\ \\\\ & \\text{such that}~~\\forall i \\in [m] ~~~\\xi_i \\geq 0 ~~\\text{and}~~ \\xi_i \\geq  1 - y_i(G \\alpha)_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "In doing so, we have added a new variable, $\\xi$, into our equations.\n",
    "However, to make our quadratic program fit the form required by\n",
    "`quadprog`, we need to have just one variable vector. Therefore, we can\n",
    "define our variable vector, $x$, to be the $\\alpha$ vector concatenated\n",
    "with the $\\xi$ vector. Recall that `quadprog` expects our quadratic\n",
    "program to fit the following form.\n",
    "$$\\text{Minimize }\\frac{1}{2}x^{\\text{T}}Qx + c^{\\text{T}}x$$\n",
    "$$\\text{such that }Ax \\leq b$$\n",
    "Tips:\n",
    "\n",
    "-   We know that $x$ should be $\\alpha$ concatenated with $\\xi$ (this\n",
    "    should be length $2m$)\n",
    "\n",
    "-   You can turn greater-than inequalities into less-than inequalities\n",
    "    by multiplying them by -1, $-Ax \\geq -b$\n",
    "\n",
    "-   Remember to set the appropriate values of $Q,c,A$ and $b$ to zero so\n",
    "    that the input to `quadprog` is equivalent to the soft SVM\n",
    "    objective.\n",
    "\n",
    "You can optionally fill in the matrices and vectors as shown below before you start coding. \n",
    "This should help you visualize the problem.\n",
    "\n",
    "-   Objective Function:\n",
    "![image](objective_function.png)\n",
    "-   Inequality_constraint:\n",
    "![image](inequality_constraint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classification**\n",
    "\n",
    "After solving for the $\\alpha$ values, we can classify a given data\n",
    "point $x$ using the following expression:\n",
    "\n",
    "$$h(x) = \\text{sign} \\left( \\sum_{i=1}^m \\alpha_i K(x_i, x)\\right)$$\n",
    "\n",
    "where the sign function is defined as usual. <br>\n",
    "*Note: You can assign the output of $\\text{sign}(0)$ to either -1 or 1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kernels**\n",
    "\n",
    "We use the kernel trick to implicitly transform the data to higher\n",
    "dimensions. Two popular kernels for SVMs are the polynomial kernel and\n",
    "the radial basis function kernel. Recall that a kernel function can be\n",
    "used to compute an inner product in high dimensions, so you can replace\n",
    "$\\langle \\Psi(x_i),\\Psi(x_j) \\rangle$ with $K(x_i, x_j)$, which takes a\n",
    "different form depending on the kernel used:\n",
    "\n",
    "-   **Linear Kernel** (the same as the dot product):\n",
    "    $$K(x_i, x_j) = x_{i}^{T}x_j.$$\n",
    "\n",
    "-   **Polynomial Kernel**: $$K(x_i,x_j) = (x_{i}^{T}x_j+c)^k.$$\n",
    "\n",
    "-   **Radial Basis Function Kernel**:\n",
    "    $$K(x_i,x_j) = \\exp\\left(-\\gamma \\vert\\vert x_i - x_j\\vert\\vert^2\\right).$$\n",
    "\n",
    "\n",
    "Note that $c$, $k$ and $\\gamma$ are all *hyperparameters* here. That is,\n",
    "the kernels will behave differently depending on the values you use for\n",
    "these parameters. For example, as $d$ increases, the polynomial kernel\n",
    "can represent more flexible decision boundaries. The RBF kernel above is\n",
    "also a more generalized version than then one presented in lecture,\n",
    "holding a $\\gamma$ parameter. We provide you with hyperparameter values\n",
    "that perform well, and do not expect you to change them; however, feel\n",
    "free to experiment yourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **QP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import quadprog\n",
    "\n",
    "\n",
    "def solve_QP(Q, c, A, b, E=None, d=None):\n",
    "    \"\"\"\n",
    "        Solves the following quadratic program:\n",
    "        minimize (1/2)x^T Q x + c^T x\n",
    "        subject to Ax <= b and Ex=d\n",
    "        for purpose of this assignment, E and d are NOT used.\n",
    "        (Adapted from: https://scaron.info/blog/quadratic-programming-in-python.html)\n",
    "\n",
    "        @param Q 2D Numpy matrix in the equation above\n",
    "        @param c 1D Numpy matrix in the equation above\n",
    "        @param A 2D Numpy matrix in the equation above\n",
    "        @param b 1D Numpy matrix in the equation above\n",
    "        @param E 2D Numpy matrix in the equation above\n",
    "        @param d 1D  Numpy matrix in the equation above\n",
    "        \n",
    "        @return 1D Numpy array contaning the values of the variables in the optimal solution\n",
    "    \"\"\"\n",
    "\n",
    "    # Perturb Q so it is positive definite\n",
    "    qp_G = Q + 10 ** (-9) * np.identity(Q.shape[0])\n",
    "\n",
    "    qp_a = -c\n",
    "    if E is not None:\n",
    "        qp_C = -np.vstack([E, A]).T\n",
    "        qp_b = -np.hstack([d.T, b.T])\n",
    "        meq = E.shape[0]\n",
    "    else:  # no equality constraint\n",
    "        qp_C = -A.T\n",
    "        qp_b = -b\n",
    "        meq = 0\n",
    "\n",
    "    return quadprog.solve_qp(qp_G.astype(np.float64), qp_a.astype(np.float64), qp_C.astype(np.float64), qp_b.astype(np.float64), meq)[0]\n",
    "\n",
    "\n",
    "def qp_example():\n",
    "    \"\"\"\n",
    "    The only purpose of this example is to demonstrate how to use the QP solver.\n",
    "    Solves the example available here: https://scaron.info/blog/quadratic-programming-in-python.html\n",
    "    \"\"\"\n",
    "    Q = np.array([[6., 3.], [1., 2.]])\n",
    "    c = np.array([-6., 4.]).reshape((2,))\n",
    "    A = np.array([[-3., 1.], [0., 2.], [-1., 1.], [-1., 0.], [0., -1.]])\n",
    "    b = np.array([-5., 6., -1., 0., 0.]).reshape((5,))\n",
    "    print(solve_QP(Q, c, A, b))\n",
    "\n",
    "\n",
    "# Run the example\n",
    "qp_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(xi, xj):\n",
    "    \"\"\"\n",
    "    Kernel Function, linear kernel (ie: regular dot product)\n",
    "    @param xi: an input sample (1D np array)\n",
    "    @param xj: an input sample (1D np array)\n",
    "    @return: float64\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "def rbf_kernel(xi, xj, gamma=0.1):\n",
    "    \"\"\"\n",
    "    Kernel Function, radial basis function kernel\n",
    "    @param xi: an input sample (1D np array)\n",
    "    @param xj: an input sample (1D np array)\n",
    "    @param gamma: parameter of the RBF kernel (scalar)\n",
    "    @return: float64\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "def polynomial_kernel(xi, xj, c=2, d=2):\n",
    "    \"\"\"\n",
    "    Kernel Function, polynomial kernel\n",
    "    @param xi: an input sample (1D np array)\n",
    "    @param xj: an input sample (1D np array)\n",
    "    @param c: mean of the polynomial kernel (scalar)\n",
    "    @param d: exponent of the polynomial (scalar)\n",
    "    @return: float64\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel_func=linear_kernel, lambda_param=.1):\n",
    "        self.kernel_func = kernel_func\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def train(self, inputs, labels):\n",
    "        \"\"\"\n",
    "        train the model with the input data (inputs and labels),\n",
    "        find the coefficients and constaints for the quadratic program and\n",
    "        calculate the alphas\n",
    "        @param inputs: inputs of data, a numpy array\n",
    "        @param labels: labels of data, a numpy array\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        self.train_inputs = inputs\n",
    "        self.train_labels = labels\n",
    "\n",
    "        # constructing QP variables\n",
    "        G = self._get_gram_matrix()\n",
    "        Q, c = self._objective_function(G)\n",
    "        A, b = self._inequality_constraint(G)\n",
    "\n",
    "        # TODO: Uncomment the next line when you have implemented _get_gram_matrix(),\n",
    "        # _inequality_constraints() and _objective_function().\n",
    "        #self.alpha = solve_QP(Q, c, A, b)[:self.train_inputs.shape[0]]\n",
    "\n",
    "    def _get_gram_matrix(self):\n",
    "        \"\"\"\n",
    "        Generate the Gram matrix for the training data stored in self.train_inputs.\n",
    "        Recall that element i, j of the matrix is K(x_i, x_j), where K is the\n",
    "        kernel function.\n",
    "        @return: the Gram matrix for the training data, a numpy array\n",
    "        \"\"\"\n",
    "        # TODO \n",
    "\n",
    "        return\n",
    "\n",
    "    def _objective_function(self, G):\n",
    "        \"\"\"\n",
    "        Generate the coefficients on the variables in the objective function for the\n",
    "        SVM quadratic program.\n",
    "        Recall the objective function is:\n",
    "        minimize (1/2)x^T Q x + c^T x\n",
    "        @param G: the Gram matrix for the training data, a numpy array\n",
    "        @return: two numpy arrays, Q and c which fully specify the objective function\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "\n",
    "        return \n",
    "\n",
    "    def _inequality_constraint(self, G):\n",
    "        \"\"\"\n",
    "        Generate the inequality constraints for the SVM quadratic program. The\n",
    "        constraints will be enforced so that Ax <= b.\n",
    "        @param G: the Gram matrix for the training data, a numpy array\n",
    "        @return: two numpy arrays, A and b which fully specify the constraints\n",
    "        \"\"\"\n",
    "        # TODO (hint: you can think of x as the concatenation of all the alphas and\n",
    "        # all the all the xi's; think about what this implies for what A should look like.)\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Generate predictions given input.\n",
    "        @param input: 2D Numpy array. Each row is a vector for which we output a prediction.\n",
    "        @return: A 1D numpy array of predictions.\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "        \n",
    "        return\n",
    "\n",
    "    def accuracy(self, inputs, labels):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the classifer given inputs and their true labels.\n",
    "        @param inputs: 2D Numpy array which we are testing calculating the accuracy of.\n",
    "        @param labels: 1D Numpy array with the inputs corresponding true labels.\n",
    "        @return: A float indicating the accuracy (between 0.0 and 1.0)\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# Sets random seed for testing purposes\n",
    "np.random.seed(0)\n",
    "\n",
    "# Creates Test Models\n",
    "test_model1 = SVM()\n",
    "test_model2 = SVM()\n",
    "\n",
    "# Creates Test Data\n",
    "x = np.array([[0,4,-1], [0,3,-1], [5,0,1], [4,1,1], [0,5,-1]])\n",
    "y = np.array([-1,-1,1,1,-1])\n",
    "x_test = [[0,0,-1], [-5,3,-1], [9,0,1], [1,0,-1], [6,-7,1]]\n",
    "y_test = np.array([-1,-1,1,-1,1])\n",
    "\n",
    "x2 = np.array([[0,0,1], [0,1,1], [1,1,1], [1,1,1], [0,0,0], [1,1,0]])\n",
    "y2 = np.array([-1,1,1,1,-1,1])\n",
    "x_test2 = np.array([[0,0,1], [0,1,1], [1,1,1], [1,0,0]])\n",
    "y_test2 = np.array([-1,1,1,-1])\n",
    "\n",
    "# test the three kernel functions\n",
    "xi = x2[0]\n",
    "xj = x2[1]\n",
    "assert linear_kernel(xi, xj) == 1\n",
    "assert rbf_kernel(xi, xj) == pytest.approx(0.905, .001)\n",
    "assert polynomial_kernel(xi, xj) == 9\n",
    "\n",
    "# Test Model Train\n",
    "test_model1.train(x, y)\n",
    "test_model2.train(x2, y2)\n",
    "\n",
    "# Test _get_gram_matrix\n",
    "G2 = test_model2._get_gram_matrix() \n",
    "assert (G2.sum() == 41)\n",
    "assert G2.shape == (6, 6)\n",
    "assert G2.dtype == \"float64\"\n",
    "\n",
    "G1 = test_model1._get_gram_matrix() \n",
    "assert (G1.sum() == 251.0)\n",
    "assert G1.shape == (5, 5)\n",
    "assert G1.dtype == \"float64\"\n",
    "\n",
    "# Test _objective_function\n",
    "assert test_model2._objective_function(G2)[0].sum() == pytest.approx(8.2, .01)\n",
    "assert test_model2._objective_function(G2)[1].sum() == pytest.approx(1, .01)\n",
    "\n",
    "Q2, c2 = test_model2._objective_function(G2)\n",
    "assert Q2.shape == (12, 12)\n",
    "assert Q2.dtype == \"float64\"\n",
    "assert c2.shape == (12,)\n",
    "assert c2.dtype == \"float64\"\n",
    "\n",
    "assert test_model1._objective_function(G1)[0].sum() == pytest.approx(50.2, .01)\n",
    "assert test_model1._objective_function(G1)[1].sum() == pytest.approx(1.0, .01)\n",
    "\n",
    "Q1, c1 = test_model1._objective_function(G1)\n",
    "assert Q1.shape == (10, 10)\n",
    "assert Q1.dtype == \"float64\"\n",
    "assert c1.shape == (10,)\n",
    "assert c1.dtype == \"float64\"\n",
    "\n",
    "# Teset _inequality_constraint\n",
    "assert test_model2._inequality_constraint(G2)[0].sum() == pytest.approx(-45, .01)\n",
    "assert test_model2._inequality_constraint(G2)[1].sum() == pytest.approx(-6, .01)\n",
    "\n",
    "A2, b2 = test_model2._inequality_constraint(G2)\n",
    "assert A2.shape == (12, 12)\n",
    "assert A2.dtype == \"float64\"\n",
    "assert b2.shape == (12,)\n",
    "assert b2.dtype == \"float64\"\n",
    "\n",
    "assert test_model1._inequality_constraint(G1)[0].sum() == pytest.approx(57.0, .01)\n",
    "assert test_model1._inequality_constraint(G1)[1].sum() == pytest.approx(-5.0, .01)\n",
    "\n",
    "A1, b1 = test_model1._inequality_constraint(G1)\n",
    "assert A1.shape == (10, 10)\n",
    "assert A1.dtype == \"float64\"\n",
    "assert b1.shape == (10,)\n",
    "assert b1.dtype == \"float64\"\n",
    "\n",
    "# Test predict\n",
    "assert test_model1.alpha == pytest.approx(np.array([-0.0370, -0.0824, -0.0102, 0.0809, 0.0083]), .01)\n",
    "assert test_model2.alpha == pytest.approx(np.array([-0.5333, 0.8333, -0.15, -0.15, 0, 0.3833]), .01)\n",
    "\n",
    "assert (test_model1.predict(x_test) == np.array([-1, -1,  1,  1,  1])).all()\n",
    "assert (test_model2.predict(x_test2) == np.array([1, 1, 1, 1])).all()\n",
    "\n",
    "# Test accuracy\n",
    "assert test_model1.accuracy(x_test, y_test) == .8\n",
    "assert test_model2.accuracy(x_test2, y_test2) == .5\n",
    "\n",
    "from datetime import date\n",
    "#[TODO] Print your name with the date, using today function from date "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def test_svm(train_data, test_data, kernel_func=linear_kernel, lambda_param=.1):\n",
    "    \"\"\"\n",
    "    Create an SVM classifier with a specificied kernel_func, train it with\n",
    "    train_data and print the accuracy of model on test_data\n",
    "    @param train_data: a namedtuple including training inputs and training labels\n",
    "    @param test_data: a namedtuple including test inputs and test labels\n",
    "    @param kernel_func: kernel function to use in the SVM\n",
    "    @return: None\n",
    "    \"\"\"\n",
    "    svm_model = SVM(kernel_func=kernel_func, lambda_param=lambda_param)\n",
    "    svm_model.train(train_data.inputs, train_data.labels)\n",
    "    train_accuracy = svm_model.accuracy(train_data.inputs, train_data.labels)\n",
    "    test_accuracy = svm_model.accuracy(test_data.inputs, test_data.labels)\n",
    "    if not (train_accuracy is None):\n",
    "        print('Train accuracy: ', round(train_accuracy * 100, 2), '%')\n",
    "    if not (test_accuracy is None):\n",
    "        print('Test accuracy:', round(test_accuracy * 100,2), '%')\n",
    "    return train_accuracy,test_accuracy\n",
    "\n",
    "def read_data(file_name):\n",
    "    \"\"\"\n",
    "    Reads the data from the input file and splits it into normalized inputs \n",
    "    and labels.\n",
    "    Please do not change this function.\n",
    "    @param file_name: path to the desired data file\n",
    "    @return: two numpy arrays, one containing the inputs and one containing\n",
    "              the labels\n",
    "    \"\"\"\n",
    "    inputs, labels, classes = [], [], set()\n",
    "    with open(file_name) as f:\n",
    "        positive_label = None\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            example = np.array(row)\n",
    "            classes.add(example[-1])\n",
    "            # our datasets all start with a True example\n",
    "            if positive_label is None:\n",
    "                positive_label = example[-1]\n",
    "            # converting data points to labels of [-1, 1]\n",
    "            label = 1 if example[-1] == positive_label else -1\n",
    "            row.pop()\n",
    "            labels.append(label)\n",
    "            inputs.append([float(val) for val in row])\n",
    "\n",
    "    if len(classes) > 2:\n",
    "        print('Only binary classification tasks are supported.')\n",
    "        exit()\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Normalize the feature values\n",
    "    for j in range(inputs.shape[1]):\n",
    "        col = inputs[:,j]\n",
    "        mu = np.mean(col)\n",
    "        sigma = np.std(col)\n",
    "        if sigma == 0: sigma = 1\n",
    "        inputs[:,j] = 1/sigma * (col - mu)\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "# set random seed for deterministic behavior -- DO NOT CHANGE\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# svm_dataset = 'fake-data1'\n",
    "# svm_dataset = 'fake-data2'\n",
    "svm_dataset = 'spambase'\n",
    "#svm_dataset = 'spambase_abridged'\n",
    "\n",
    "Dataset = namedtuple('Dataset', ['inputs', 'labels'])\n",
    "\n",
    "# Build the filename\n",
    "filename = 'data/' + svm_dataset+ '.csv'\n",
    "\n",
    "# Read data\n",
    "inputs, labels = read_data(filename)\n",
    "\n",
    "print('================ ' + svm_dataset.swapcase() + ' ================')\n",
    "\n",
    "# Split data into training set and test set with a ratio of 4:1\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.20)\n",
    "\n",
    "train_data = Dataset(train_inputs, train_labels)\n",
    "test_data = Dataset(test_inputs[:], test_labels[:])\n",
    "\n",
    "print(\"Shape of training data inputs: \", train_data.inputs.shape)\n",
    "print(\"Shape of test data inputs:\", test_data.inputs.shape)\n",
    "\n",
    "m = train_data.inputs.shape[0]\n",
    "n = train_data.inputs.shape[1]\n",
    "\n",
    "# Set lambda parameter. You do not need to change this but are free to experiment.\n",
    "lambda_param = 1.0 / (2*m)\n",
    "\n",
    "print('================ Linear kernel  =================')\n",
    "test_svm(train_data, test_data, kernel_func=linear_kernel, lambda_param=lambda_param)\n",
    "print('================ RBF kernel =================')\n",
    "# Set gamma to 1/n. This matches the behavior of sklearn's implementation.\n",
    "rbf_with_gamma = lambda x, y: rbf_kernel(x, y, 1.0/n)\n",
    "test_svm(train_data, test_data, kernel_func=rbf_with_gamma, lambda_param=lambda_param)\n",
    "print('============= Polynomial kernel =============')\n",
    "test_svm(train_data, test_data, kernel_func=polynomial_kernel, lambda_param=lambda_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Report**\n",
    "\n",
    "### **Question 1** (2 points)\n",
    "Comment on the testing and training accuracy of your SVM classifier\n",
    "on the Spam dataset (NOT spambase_abridged.csv). Discuss how kernels\n",
    "(and their hyperparameters) affect the classifier's accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2** (3 points)\n",
    "Read this [NY times article](https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html)\n",
    "about algorithmic bias in law enforcement. What do you think law\n",
    "enforcement organizations can do to limit cases of wrongful\n",
    "accusations caused by biased algorithms? What can the companies that\n",
    "produce these technologies do? Consider some of the solutions hinted\n",
    "at in the article: would increasing the racial diversity in the\n",
    "images fed into the algorithm help? What about banning low-quality\n",
    "images from being used? Think of other potential solutions too. On a\n",
    "more general level, should we even be using such algorithms in law\n",
    "enforcement? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3** (5 points)\n",
    "Plot the differences in training and testing error as\n",
    "you change the hyperparameters of the polynomial and RBF kernel using the spambase dataset.\n",
    "\n",
    "Use c = 1 and p = [0,1,2,3,4] for the polynomial kernel.\n",
    "\n",
    "Use gamma = [1000/n, 100/n, 10/n, 1/n, 0.1/n, 0.01/n] where n is the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 4** (10 points)\n",
    "Produce a visualization of the decision boundaries on\n",
    "the both of the 2D datasets (`fake-data1.csv`, `fake-data2.csv`) for\n",
    "two out of the three kernel functions. Your plot should contain the training data and\n",
    "decision boundaries. In addition, the label of each training point\n",
    "should be included. This should all be contained within a single\n",
    "figure for each dataset.\n",
    "\n",
    "**Hint:** Look into [matplotlib.pyplot.contour](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html#examples-using-matplotlib-pyplot-contour) and [numpy.meshgrid](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html) to visualize the decision boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcd9bc17ffadb8b3c09124f861805f4f094648af93180b87f0218364b7d0c0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
